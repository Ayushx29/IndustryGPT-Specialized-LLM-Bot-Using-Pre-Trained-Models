{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets peft trl bitsandbytes accelerate\n!pip install -q huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T08:17:42.727006Z","iopub.execute_input":"2025-06-21T08:17:42.727350Z","iopub.status.idle":"2025-06-21T08:17:49.214891Z","shell.execute_reply.started":"2025-06-21T08:17:42.727318Z","shell.execute_reply":"2025-06-21T08:17:49.213889Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T08:17:52.672042Z","iopub.execute_input":"2025-06-21T08:17:52.672343Z","iopub.status.idle":"2025-06-21T08:18:09.032392Z","shell.execute_reply.started":"2025-06-21T08:17:52.672313Z","shell.execute_reply":"2025-06-21T08:18:09.031804Z"}},"outputs":[{"name":"stderr","text":"2025-06-21 08:18:01.415938: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750493881.615023     145 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750493881.673717     145 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"torch.cuda.is_available()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T08:19:24.774494Z","iopub.execute_input":"2025-06-21T08:19:24.774802Z","iopub.status.idle":"2025-06-21T08:19:24.779696Z","shell.execute_reply.started":"2025-06-21T08:19:24.774778Z","shell.execute_reply":"2025-06-21T08:19:24.779108Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict, concatenate_datasets\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n\n\n# Load dataset\ndataset1 = load_dataset(\"poornima9348/finance-alpaca-1k-test\")\ndataset2 = load_dataset(\"ssbuild/alpaca_finance_en\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T08:19:26.611001Z","iopub.execute_input":"2025-06-21T08:19:26.611290Z","iopub.status.idle":"2025-06-21T08:19:30.133148Z","shell.execute_reply.started":"2025-06-21T08:19:26.611269Z","shell.execute_reply":"2025-06-21T08:19:30.132597Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"dataset1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T08:19:32.118441Z","iopub.execute_input":"2025-06-21T08:19:32.119035Z","iopub.status.idle":"2025-06-21T08:19:32.123699Z","shell.execute_reply.started":"2025-06-21T08:19:32.119013Z","shell.execute_reply":"2025-06-21T08:19:32.123089Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    test: Dataset({\n        features: ['instruction', 'input', 'output', 'text'],\n        num_rows: 1000\n    })\n})"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"dataset2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T08:19:34.019598Z","iopub.execute_input":"2025-06-21T08:19:34.019901Z","iopub.status.idle":"2025-06-21T08:19:34.025085Z","shell.execute_reply.started":"2025-06-21T08:19:34.019878Z","shell.execute_reply":"2025-06-21T08:19:34.024263Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'instruction', 'input', 'output'],\n        num_rows: 68912\n    })\n})"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Combine 'instruction' and 'output' columns into a new 'text' column\ndef combine_text_columns(example):\n    return {'text': f\"{example['instruction']} ### {example['output']}\"}\n\n# Apply the function to each example in the dataset\ndataset1 = dataset1.map(combine_text_columns)\ndataset2 = dataset2.map(combine_text_columns)\n\n# Remove 'instruction', 'input' and 'output' columns\ndataset1['test']=dataset1['test'].remove_columns(['instruction','input', 'output'])\ndataset2['train']=dataset2['train'].remove_columns(['instruction','input', 'output','id'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T08:19:36.149712Z","iopub.execute_input":"2025-06-21T08:19:36.150481Z","iopub.status.idle":"2025-06-21T08:19:36.163247Z","shell.execute_reply.started":"2025-06-21T08:19:36.150429Z","shell.execute_reply":"2025-06-21T08:19:36.162363Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Perform the train-test split on the necessary dataset if required\nsplit_dataset1 = dataset1['test'].train_test_split(train_size=0.8)\nsplit_dataset2 = dataset2['train'].train_test_split(test_size=0.2)","metadata":{"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"split_dataset1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T08:19:41.620800Z","iopub.execute_input":"2025-06-21T08:19:41.621478Z","iopub.status.idle":"2025-06-21T08:19:41.626041Z","shell.execute_reply.started":"2025-06-21T08:19:41.621446Z","shell.execute_reply":"2025-06-21T08:19:41.625404Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 800\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 200\n    })\n})"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"split_dataset2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T08:19:44.223683Z","iopub.execute_input":"2025-06-21T08:19:44.224184Z","iopub.status.idle":"2025-06-21T08:19:44.228933Z","shell.execute_reply.started":"2025-06-21T08:19:44.224162Z","shell.execute_reply":"2025-06-21T08:19:44.228303Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 55129\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 13783\n    })\n})"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# Concatenate the datasets\nmerged_train = concatenate_datasets([split_dataset1['train'], split_dataset2['train']])\nmerged_test = concatenate_datasets([split_dataset1['test'], split_dataset2['test']])\n\n# Create a new DatasetDict with the merged datasets\nmerged_dataset = DatasetDict({\n    'train': merged_train,\n    'test': merged_test\n})\n\n# Filter out None values in case some splits are missing\nmerged_dataset = DatasetDict({k: v for k, v in merged_dataset.items() if v is not None})\n\n# Print the merged dataset to verify\nprint(merged_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T08:19:47.510616Z","iopub.execute_input":"2025-06-21T08:19:47.511130Z","iopub.status.idle":"2025-06-21T08:19:47.524547Z","shell.execute_reply.started":"2025-06-21T08:19:47.511104Z","shell.execute_reply":"2025-06-21T08:19:47.523854Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 55929\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 13983\n    })\n})\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Shuffle the dataset and slice it\nmerged_train_dataset = merged_dataset['train'].shuffle(seed=42).select(range(5000))\n\ndef transform_conversation(example):\n    conversation_text1 = example['text']\n    segments = conversation_text1.split('###')\n\n    reformatted_segments = []\n\n    # Iterate over the segments and ensure each segment has a prompt and answer\n    for i in range(0, len(segments) - 1, 2):\n        prompt = segments[i].strip()\n        if i + 1 < len(segments):\n            answer = segments[i + 1].strip()\n            # Apply the new template\n            reformatted_segments.append(f'<s>[INST] {prompt} [/INST] {answer} </s>')\n        else:\n            # Handle the case where there is no corresponding assistant segment\n            reformatted_segments.append(f'<s>[INST] {prompt} [/INST] </s>')\n\n    return {'text': ''.join(reformatted_segments)}\n\n# Apply the transformation\ntransformed_dataset = merged_train_dataset.map(transform_conversation)","metadata":{"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31b1a818c6d44ae38e457aabaa58d71e"}},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# Shuffle the dataset and slice it\nmerged_test_dataset = merged_dataset['test'].shuffle(seed=42).select(range(100))\n\ndef transform_conversation(example):\n    conversation_text1 = example['text']\n    segments = conversation_text1.split('###')\n\n    reformatted_segments = []\n\n    # Iterate over the segments and ensure each segment has a prompt and answer\n    for i in range(0, len(segments) - 1, 2):\n        prompt = segments[i].strip()\n        if i + 1 < len(segments):\n            answer = segments[i + 1].strip()\n            # Apply the new template\n            reformatted_segments.append(f'<s>[INST] {prompt} [/INST] {answer} </s>')\n        else:\n            # Handle the case where there is no corresponding assistant segment\n            reformatted_segments.append(f'<s>[INST] {prompt} [/INST] </s>')\n\n    return {'text': ''.join(reformatted_segments)}\n\n# Apply the transformation\ntransformed_test_dataset = merged_test_dataset.map(transform_conversation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T08:19:56.152958Z","iopub.execute_input":"2025-06-21T08:19:56.153603Z","iopub.status.idle":"2025-06-21T08:19:56.191548Z","shell.execute_reply.started":"2025-06-21T08:19:56.153578Z","shell.execute_reply":"2025-06-21T08:19:56.190823Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff48b6e3bf8f498486c9b025966c0e4a"}},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"merged_test_dataset['text'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T08:19:58.864172Z","iopub.execute_input":"2025-06-21T08:19:58.864497Z","iopub.status.idle":"2025-06-21T08:19:58.872632Z","shell.execute_reply.started":"2025-06-21T08:19:58.864473Z","shell.execute_reply":"2025-06-21T08:19:58.872021Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"'You are given a list of words, sort them alphabetically ### [\"ant\", \"bat\", \"cat\", \"dog\", \"monkey\"]'"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    BitsAndBytesConfig,\n    DataCollatorForSeq2Seq\n)\nfrom peft import LoraConfig\nfrom trl import SFTTrainer\n\n# -------------------- CONFIGURATION --------------------\n\n# Model and output\nmodel_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nnew_model = \"tinyllama-finance-chatbot-finetune\"\n\n# QLoRA parameters\nlora_r = 64\nlora_alpha = 16\nlora_dropout = 0.1\n\n# BitsAndBytes config\nuse_4bit = True\nbnb_4bit_compute_dtype = \"float16\"\nbnb_4bit_quant_type = \"nf4\"\nuse_nested_quant = False\n\n# Training config\noutput_dir = \"./results\"\nnum_train_epochs = 1\nfp16 = False\nbf16 = False\nper_device_train_batch_size = 4\nper_device_eval_batch_size = 4\ngradient_accumulation_steps = 1\ngradient_checkpointing = True\nmax_grad_norm = 0.3\nlearning_rate = 2e-4\nweight_decay = 0.001\noptim = \"paged_adamw_32bit\"\nlr_scheduler_type = \"cosine\"\nmax_steps = -1\nwarmup_ratio = 0.03\ngroup_by_length = True\nsave_steps = 0\nlogging_steps = 25\nmax_seq_length = 350\n\n# -------------------- TOKENIZER & DATA --------------------\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\ndef tokenize(example):\n    encodings = tokenizer(\n        example[\"text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=max_seq_length,\n        return_attention_mask=True,\n    )\n    return {\n        \"input_ids\": encodings[\"input_ids\"],\n        \"attention_mask\": encodings[\"attention_mask\"],\n        \"labels\": encodings[\"input_ids\"]\n    }\n\n# Assuming `transformed_dataset` is already defined\ntokenized_dataset = transformed_dataset.map(\n    tokenize,\n    batched=True,\n    remove_columns=transformed_dataset.column_names,\n    load_from_cache_file=False\n)\n\n# Set torch format for compatibility\ntokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\nprint(\"âœ… Tokenized keys:\", tokenized_dataset[0].keys())\n\n# -------------------- MEMORY MANAGEMENT --------------------\n\ntorch.cuda.empty_cache()\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nos.makedirs(\"/kaggle/working/offload\", exist_ok=True)\n\n# -------------------- LOAD MODEL --------------------\n\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    offload_folder=\"/kaggle/working/offload\",\n    offload_buffers=True\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# -------------------- PEFT CONFIG --------------------\n\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# -------------------- TRAINING ARGS --------------------\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\"\n)\n\n# -------------------- DATA COLLATOR --------------------\n\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    padding=\"max_length\",\n    max_length=max_seq_length,\n    return_tensors=\"pt\"\n)\n\n# -------------------- SFT TRAINER --------------------\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=tokenized_dataset,\n    peft_config=peft_config,\n    args=training_arguments,\n    data_collator=data_collator\n)\n\n# -------------------- TRAIN --------------------\n\ntrainer.train()\nprint(\"ðŸŽ‰ Training complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T09:13:58.427847Z","iopub.execute_input":"2025-06-21T09:13:58.428833Z","iopub.status.idle":"2025-06-21T09:36:35.820571Z","shell.execute_reply.started":"2025-06-21T09:13:58.428799Z","shell.execute_reply":"2025-06-21T09:36:35.819923Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6892217bf8524c4dbddfbc5405f95566"}},"metadata":{}},{"name":"stdout","text":"âœ… Tokenized keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35604d410aeb48c7927b0ea1cdf55449"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1250/1250 22:28, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>3.467600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.913900</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.645200</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.778100</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.713900</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.683600</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.580900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.635400</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.757400</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.687500</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.760900</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.680600</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>0.651800</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.657300</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>0.775400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.619500</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>0.593800</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.696000</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>0.676100</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.642600</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>0.690900</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.766100</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>0.627300</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.641400</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>0.596200</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.551000</td>\n    </tr>\n    <tr>\n      <td>675</td>\n      <td>0.614700</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.618100</td>\n    </tr>\n    <tr>\n      <td>725</td>\n      <td>0.666400</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.610000</td>\n    </tr>\n    <tr>\n      <td>775</td>\n      <td>0.686800</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.660600</td>\n    </tr>\n    <tr>\n      <td>825</td>\n      <td>0.680500</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.619200</td>\n    </tr>\n    <tr>\n      <td>875</td>\n      <td>0.718300</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.774500</td>\n    </tr>\n    <tr>\n      <td>925</td>\n      <td>0.571100</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.666100</td>\n    </tr>\n    <tr>\n      <td>975</td>\n      <td>0.639200</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.660500</td>\n    </tr>\n    <tr>\n      <td>1025</td>\n      <td>0.664100</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.750500</td>\n    </tr>\n    <tr>\n      <td>1075</td>\n      <td>0.721200</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.505400</td>\n    </tr>\n    <tr>\n      <td>1125</td>\n      <td>0.727500</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.739600</td>\n    </tr>\n    <tr>\n      <td>1175</td>\n      <td>0.620800</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.691400</td>\n    </tr>\n    <tr>\n      <td>1225</td>\n      <td>0.622700</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.508900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"ðŸŽ‰ Training complete!\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"# Save trained model\ntrainer.model.save_pretrained(new_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T09:39:06.979363Z","iopub.execute_input":"2025-06-21T09:39:06.980243Z","iopub.status.idle":"2025-06-21T09:39:07.485426Z","shell.execute_reply.started":"2025-06-21T09:39:06.980217Z","shell.execute_reply":"2025-06-21T09:39:07.484868Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"# List the contents to ensure files are saved\nprint(\"Contents of new_model directory:\", os.listdir(new_model))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T09:39:32.312126Z","iopub.execute_input":"2025-06-21T09:39:32.312456Z","iopub.status.idle":"2025-06-21T09:39:32.317379Z","shell.execute_reply.started":"2025-06-21T09:39:32.312416Z","shell.execute_reply":"2025-06-21T09:39:32.316621Z"}},"outputs":[{"name":"stdout","text":"Contents of new_model directory: ['adapter_config.json', 'adapter_model.safetensors', 'README.md']\n","output_type":"stream"}],"execution_count":75},{"cell_type":"code","source":"# Ignore warnings\nimport logging\nfrom transformers import pipeline\nfrom transformers.utils import logging\nlogging.set_verbosity(logging.CRITICAL)\n\n\n# Run text generation pipeline with our fine-tuned model\nprompt = \"Generate a title for a blog about the Nobel Prize ceremony.\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=100)\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T09:41:22.324694Z","iopub.execute_input":"2025-06-21T09:41:22.325262Z","iopub.status.idle":"2025-06-21T09:41:23.107841Z","shell.execute_reply.started":"2025-06-21T09:41:22.325237Z","shell.execute_reply":"2025-06-21T09:41:23.107145Z"}},"outputs":[{"name":"stdout","text":"<s>[INST] Generate a title for a blog about the Nobel Prize ceremony. [/INST] The Nobel Prize Ceremony: A Tribute to Innovation \n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"# TensorBoard extension (if applicable)\n%load_ext tensorboard\n%tensorboard --logdir results/runs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T09:41:55.374763Z","iopub.execute_input":"2025-06-21T09:41:55.375292Z","iopub.status.idle":"2025-06-21T09:42:01.938461Z","shell.execute_reply.started":"2025-06-21T09:41:55.375267Z","shell.execute_reply":"2025-06-21T09:42:01.937816Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        (async () => {\n            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n            url.searchParams.set('tensorboardColab', 'true');\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '800');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    "},"metadata":{}}],"execution_count":78},{"cell_type":"code","source":"# Reload model and tokenizer (if needed) and merge LoRA\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nimport torch\nimport os\nimport shutil\n\n# Define model_name and new_model\nmodel_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nnew_model = \"tinyllama-finance-chatbot-finetune\"\n\n# Clear GPU memory\ntorch.cuda.empty_cache()\n\n# Ensure the directory exists\nif not os.path.exists(new_model):\n    os.makedirs(new_model)\n\n# Define offload directory\noffload_dir = \"/kaggle/working/\"\nos.makedirs(offload_dir, exist_ok=True)\n\ntry:\n    # Load base model and merge with LoRA\n    base_model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        low_cpu_mem_usage=True,\n        return_dict=True,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        offload_folder=offload_dir\n    )\n\n    model = PeftModel.from_pretrained(base_model, new_model)\n    model = model.merge_and_unload()\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"\n\n    model.save_pretrained(new_model)\n    tokenizer.save_pretrained(new_model)\n\n    print(\"Contents of new_model directory:\", os.listdir(new_model))\n\nexcept RuntimeError as e:\n    if \"out of memory\" in str(e):\n        print(\"Out of memory error. Try using a smaller model or increasing GPU memory.\")\n        torch.cuda.empty_cache()\n    else:\n        raise e\nexcept ValueError as e:\n    print(f\"ValueError: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T09:42:45.118321Z","iopub.execute_input":"2025-06-21T09:42:45.118937Z","iopub.status.idle":"2025-06-21T09:42:52.677027Z","shell.execute_reply.started":"2025-06-21T09:42:45.118909Z","shell.execute_reply":"2025-06-21T09:42:52.676355Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3391: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving checkpoint shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff348b8012224442a8f7f8b88a62f70b"}},"metadata":{}},{"name":"stdout","text":"Contents of new_model directory: ['tokenizer.json', 'adapter_config.json', 'special_tokens_map.json', 'tokenizer_config.json', 'adapter_model.safetensors', 'config.json', 'tokenizer.model', 'model.safetensors', 'generation_config.json', 'README.md']\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"# Hugging Face Hub authentication\nimport locale\nlocale.getpreferredencoding = lambda: \"UTF-8\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T09:44:28.143120Z","iopub.execute_input":"2025-06-21T09:44:28.143945Z","iopub.status.idle":"2025-06-21T09:44:28.147596Z","shell.execute_reply.started":"2025-06-21T09:44:28.143917Z","shell.execute_reply":"2025-06-21T09:44:28.146809Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\n# âœ… Retrieve token using your actual secret name\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"llm_training\")\n\n# Login and push to hub\nlogin(token=hf_token)\n\nmodel_repo_name = \"Ayushx29/finance_finetune_model\"\ntokenizer_repo_name = \"Ayushx29/finance_finetune_model\"\n\nmodel.push_to_hub(model_repo_name, use_auth_token=hf_token, check_pr=True)\ntokenizer.push_to_hub(tokenizer_repo_name, use_auth_token=hf_token, check_pr=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T09:47:58.722571Z","iopub.execute_input":"2025-06-21T09:47:58.723183Z","iopub.status.idle":"2025-06-21T09:48:38.155619Z","shell.execute_reply.started":"2025-06-21T09:47:58.723157Z","shell.execute_reply":"2025-06-21T09:48:38.154971Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:920: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3391: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving checkpoint shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd67159c04a644f488b0b90631e8d6a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Uploading...:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0483da08f3ef4451919cdd11e0d0230e"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:920: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c605d67d6daa44ccaf871cb86a4f77dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Uploading...:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bf7419023a3493da6faf2634f684850"}},"metadata":{}},{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Ayushx29/finance_finetune_model/commit/7359821bd1cb6f50c12fb48733784a5be4b05035', commit_message='Upload tokenizer', commit_description='', oid='7359821bd1cb6f50c12fb48733784a5be4b05035', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Ayushx29/finance_finetune_model', endpoint='https://huggingface.co', repo_type='model', repo_id='Ayushx29/finance_finetune_model'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":82},{"cell_type":"code","source":"# Clear GPU memory\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T09:48:46.174132Z","iopub.execute_input":"2025-06-21T09:48:46.174597Z","iopub.status.idle":"2025-06-21T09:48:46.179403Z","shell.execute_reply.started":"2025-06-21T09:48:46.174572Z","shell.execute_reply":"2025-06-21T09:48:46.178899Z"}},"outputs":[],"execution_count":83},{"cell_type":"code","source":"# Load model back from hub (for inference/verification)\nfine_tuned_finance_model = AutoModelForCausalLM.from_pretrained(\"Ayushx29/finance_finetune_model\")\nfine_tuned_tokenizer = AutoTokenizer.from_pretrained(\"Ayushx29/finance_finetune_model\", trust_remote_code=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T09:49:10.574541Z","iopub.execute_input":"2025-06-21T09:49:10.574804Z","iopub.status.idle":"2025-06-21T09:49:35.523955Z","shell.execute_reply.started":"2025-06-21T09:49:10.574787Z","shell.execute_reply":"2025-06-21T09:49:35.523342Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/674 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29eb78b93c114f768f143d24364b49f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27dbbcab13fd478d9686645b4fe3e33a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f41a60c971c64fa4ae3e09fa0bf5ae30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af5db920edd4413da7811cdf28889847"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30f49900a69844e185dc796906a3beee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.62M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"194e9ae2cc5d4d60a4a079b6a642f60b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1b2c870172343598ddff3938442b04c"}},"metadata":{}}],"execution_count":84},{"cell_type":"code","source":"import os\nimport shutil\n\n# Define the source (output) directory and the new target directory\nsource_dir = \"/kaggle/working/tinyllama-finance-chatbot-finetune\"\ntarget_dir = \"/kaggle/working/new_dir\"\nos.makedirs(target_dir, exist_ok=True)\n\nfiles_to_copy = [\n    \"config.json\",\n    \"tokenizer.json\",\n    \"tokenizer_config.json\",\n    \"adapter_model.safetensors\",\n    \"generation_config.json\",\n    \"tokenizer.model\",\n    \"special_tokens_map.json\"\n]\nfor file_name in files_to_copy:\n    src = os.path.join(source_dir, file_name)\n    dst = os.path.join(target_dir, file_name)\n    if os.path.exists(src):\n        shutil.copy2(src, dst)\n        print(f\"Copied {file_name} to {target_dir}\")\n    else:\n        print(f\"{file_name} not found in {source_dir}\")\n\nprint(\"âœ… File copying complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T09:55:41.559426Z","iopub.execute_input":"2025-06-21T09:55:41.560057Z","iopub.status.idle":"2025-06-21T09:55:41.947903Z","shell.execute_reply.started":"2025-06-21T09:55:41.560035Z","shell.execute_reply":"2025-06-21T09:55:41.947117Z"}},"outputs":[{"name":"stdout","text":"Copied config.json to /kaggle/working/new_dir\nCopied tokenizer.json to /kaggle/working/new_dir\nCopied tokenizer_config.json to /kaggle/working/new_dir\nCopied adapter_model.safetensors to /kaggle/working/new_dir\nCopied generation_config.json to /kaggle/working/new_dir\nCopied tokenizer.model to /kaggle/working/new_dir\nCopied special_tokens_map.json to /kaggle/working/new_dir\nâœ… File copying complete.\n","output_type":"stream"}],"execution_count":89}]}