{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2DU-T4PS_CyV"
      ]
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - IndustryGPT: Specialized LLM Bot Using Pre-Trained Models\n"
      ],
      "metadata": {
        "id": "6A1-si119YnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Deep Learning for NLP\n",
        "##### **Contribution**    - Individual\n",
        "##### **Name -** Ayush Bhagat"
      ],
      "metadata": {
        "id": "oZwfNhDq9YZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "N7wGIWer9YKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A domain-specific chatbot was developed for the Finance industry by fine-tuning the TinyLlama-1.1B-Chat-v1.0 model using QLoRA. It can answer queries related to budgeting, investments, and financial planning with improved accuracy and relevance."
      ],
      "metadata": {
        "id": "IPiQNpeq94RS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "abPcPpS0-WNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Ayushx29/IndustryGPT-Specialized-LLM-Bot-Using-Pre-Trained-Models"
      ],
      "metadata": {
        "id": "F4x29PU9-XN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "Uh8xsuEG-fJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most general-purpose chatbots struggle to provide accurate and context-aware responses in specialized fields like finance. This leads to vague or incorrect guidance on important topics such as budgeting, investing, and financial planning. There is a growing demand for compact, efficient, and domain-adapted language models that can deliver reliable financial insights while being resource-friendly and easy to integrate into user-facing applications.\n"
      ],
      "metadata": {
        "id": "fYzTbxFQ-j9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "2DU-T4PS_CyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iCyn3EnV_Djf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "rYjT5dOf_G8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets peft trl bitsandbytes accelerate\n",
        "!pip install -q huggingface_hub"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-21T08:17:42.727006Z",
          "iopub.execute_input": "2025-06-21T08:17:42.727350Z",
          "iopub.status.idle": "2025-06-21T08:17:49.214891Z",
          "shell.execute_reply.started": "2025-06-21T08:17:42.727318Z",
          "shell.execute_reply": "2025-06-21T08:17:49.213889Z"
        },
        "id": "kWkp9GAf9Uns"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-21T08:17:52.672042Z",
          "iopub.execute_input": "2025-06-21T08:17:52.672343Z",
          "iopub.status.idle": "2025-06-21T08:18:09.032392Z",
          "shell.execute_reply.started": "2025-06-21T08:17:52.672313Z",
          "shell.execute_reply": "2025-06-21T08:18:09.031804Z"
        },
        "id": "ZB1dHhPZ9Unx",
        "outputId": "e7b8408f-aaf6-4c86-a5ff-168312db42d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-06-21 08:18:01.415938: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750493881.615023     145 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750493881.673717     145 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-21T08:19:24.774494Z",
          "iopub.execute_input": "2025-06-21T08:19:24.774802Z",
          "iopub.status.idle": "2025-06-21T08:19:24.779696Z",
          "shell.execute_reply.started": "2025-06-21T08:19:24.774778Z",
          "shell.execute_reply": "2025-06-21T08:19:24.779108Z"
        },
        "id": "2bnbOw4y9Un1",
        "outputId": "944c93fe-0d37-480f-d903-40d352280a2b"
      },
      "outputs": [
        {
          "execution_count": 10,
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "dataset1 = load_dataset(\"poornima9348/finance-alpaca-1k-test\")\n",
        "dataset2 = load_dataset(\"ssbuild/alpaca_finance_en\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-21T08:19:26.611001Z",
          "iopub.execute_input": "2025-06-21T08:19:26.611290Z",
          "iopub.status.idle": "2025-06-21T08:19:30.133148Z",
          "shell.execute_reply.started": "2025-06-21T08:19:26.611269Z",
          "shell.execute_reply": "2025-06-21T08:19:30.132597Z"
        },
        "id": "qcaSCd_y9Un2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset1"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-21T08:19:32.118441Z",
          "iopub.execute_input": "2025-06-21T08:19:32.119035Z",
          "iopub.status.idle": "2025-06-21T08:19:32.123699Z",
          "shell.execute_reply.started": "2025-06-21T08:19:32.119013Z",
          "shell.execute_reply": "2025-06-21T08:19:32.123089Z"
        },
        "id": "TKmgm8rp9Un3",
        "outputId": "0b13bcdb-8c52-47f8-fda1-13466ffa3dc6"
      },
      "outputs": [
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    test: Dataset({\n        features: ['instruction', 'input', 'output', 'text'],\n        num_rows: 1000\n    })\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset2"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-21T08:19:34.019598Z",
          "iopub.execute_input": "2025-06-21T08:19:34.019901Z",
          "iopub.status.idle": "2025-06-21T08:19:34.025085Z",
          "shell.execute_reply.started": "2025-06-21T08:19:34.019878Z",
          "shell.execute_reply": "2025-06-21T08:19:34.024263Z"
        },
        "id": "2cW0FTnJ9Un4",
        "outputId": "85b2e7e4-c74a-4364-b5b7-d52184cf2abb"
      },
      "outputs": [
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'instruction', 'input', 'output'],\n        num_rows: 68912\n    })\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine 'instruction' and 'output' columns into a new 'text' column\n",
        "def combine_text_columns(example):\n",
        "    return {'text': f\"{example['instruction']} ### {example['output']}\"}\n",
        "\n",
        "# Apply the function to each example in the dataset\n",
        "dataset1 = dataset1.map(combine_text_columns)\n",
        "dataset2 = dataset2.map(combine_text_columns)\n",
        "\n",
        "# Remove 'instruction', 'input' and 'output' columns\n",
        "dataset1['test']=dataset1['test'].remove_columns(['instruction','input', 'output'])\n",
        "dataset2['train']=dataset2['train'].remove_columns(['instruction','input', 'output','id'])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-21T08:19:36.149712Z",
          "iopub.execute_input": "2025-06-21T08:19:36.150481Z",
          "iopub.status.idle": "2025-06-21T08:19:36.163247Z",
          "shell.execute_reply.started": "2025-06-21T08:19:36.150429Z",
          "shell.execute_reply": "2025-06-21T08:19:36.162363Z"
        },
        "id": "EBM6UFmw9Un5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the train-test split on the necessary dataset if required\n",
        "split_dataset1 = dataset1['test'].train_test_split(train_size=0.8)\n",
        "split_dataset2 = dataset2['train'].train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "trusted": true,
        "id": "DgDvow4j9Un6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset1"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-21T08:19:41.620800Z",
          "iopub.execute_input": "2025-06-21T08:19:41.621478Z",
          "iopub.status.idle": "2025-06-21T08:19:41.626041Z",
          "shell.execute_reply.started": "2025-06-21T08:19:41.621446Z",
          "shell.execute_reply": "2025-06-21T08:19:41.625404Z"
        },
        "id": "Gtk9BoCZ9Un8",
        "outputId": "2551ebd0-0a19-4c22-a686-89692c1cea4e"
      },
      "outputs": [
        {
          "execution_count": 16,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 800\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 200\n    })\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset2"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-21T08:19:44.223683Z",
          "iopub.execute_input": "2025-06-21T08:19:44.224184Z",
          "iopub.status.idle": "2025-06-21T08:19:44.228933Z",
          "shell.execute_reply.started": "2025-06-21T08:19:44.224162Z",
          "shell.execute_reply": "2025-06-21T08:19:44.228303Z"
        },
        "id": "fhBnwhZR9Un9",
        "outputId": "78d1176e-f1b4-4589-910e-e6ad9b9510bf"
      },
      "outputs": [
        {
          "execution_count": 17,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 55129\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 13783\n    })\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate the datasets\n",
        "merged_train = concatenate_datasets([split_dataset1['train'], split_dataset2['train']])\n",
        "merged_test = concatenate_datasets([split_dataset1['test'], split_dataset2['test']])\n",
        "\n",
        "# Create a new DatasetDict with the merged datasets\n",
        "merged_dataset = DatasetDict({\n",
        "    'train': merged_train,\n",
        "    'test': merged_test\n",
        "})\n",
        "\n",
        "# Filter out None values in case some splits are missing\n",
        "merged_dataset = DatasetDict({k: v for k, v in merged_dataset.items() if v is not None})\n",
        "\n",
        "# Print the merged dataset to verify\n",
        "print(merged_dataset)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-21T08:19:47.510616Z",
          "iopub.execute_input": "2025-06-21T08:19:47.511130Z",
          "iopub.status.idle": "2025-06-21T08:19:47.524547Z",
          "shell.execute_reply.started": "2025-06-21T08:19:47.511104Z",
          "shell.execute_reply": "2025-06-21T08:19:47.523854Z"
        },
        "id": "c7SwWVm79Un-",
        "outputId": "587df752-5515-439d-c2ba-a260a134e68e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 55929\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 13983\n    })\n})\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the dataset and slice it\n",
        "merged_train_dataset = merged_dataset['train'].shuffle(seed=42).select(range(5000))\n",
        "\n",
        "def transform_conversation(example):\n",
        "    conversation_text1 = example['text']\n",
        "    segments = conversation_text1.split('###')\n",
        "\n",
        "    reformatted_segments = []\n",
        "\n",
        "    # Iterate over the segments and ensure each segment has a prompt and answer\n",
        "    for i in range(0, len(segments) - 1, 2):\n",
        "        prompt = segments[i].strip()\n",
        "        if i + 1 < len(segments):\n",
        "            answer = segments[i + 1].strip()\n",
        "            # Apply the new template\n",
        "            reformatted_segments.append(f'<s>[INST] {prompt} [/INST] {answer} </s>')\n",
        "        else:\n",
        "            # Handle the case where there is no corresponding assistant segment\n",
        "            reformatted_segments.append(f'<s>[INST] {prompt} [/INST] </s>')\n",
        "\n",
        "    return {'text': ''.join(reformatted_segments)}\n",
        "\n",
        "# Apply the transformation\n",
        "transformed_dataset = merged_train_dataset.map(transform_conversation)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "31b1a818c6d44ae38e457aabaa58d71e"
          ]
        },
        "id": "bSujUsGb9Un-",
        "outputId": "74255978-19be-4850-f85f-2c9bdf3664c5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31b1a818c6d44ae38e457aabaa58d71e"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the dataset and slice it\n",
        "merged_test_dataset = merged_dataset['test'].shuffle(seed=42).select(range(100))\n",
        "\n",
        "def transform_conversation(example):\n",
        "    conversation_text1 = example['text']\n",
        "    segments = conversation_text1.split('###')\n",
        "\n",
        "    reformatted_segments = []\n",
        "\n",
        "    # Iterate over the segments and ensure each segment has a prompt and answer\n",
        "    for i in range(0, len(segments) - 1, 2):\n",
        "        prompt = segments[i].strip()\n",
        "        if i + 1 < len(segments):\n",
        "            answer = segments[i + 1].strip()\n",
        "            # Apply the new template\n",
        "            reformatted_segments.append(f'<s>[INST] {prompt} [/INST] {answer} </s>')\n",
        "        else:\n",
        "            # Handle the case where there is no corresponding assistant segment\n",
        "            reformatted_segments.append(f'<s>[INST] {prompt} [/INST] </s>')\n",
        "\n",
        "    return {'text': ''.join(reformatted_segments)}\n",
        "\n",
        "# Apply the transformation\n",
        "transformed_test_dataset = merged_test_dataset.map(transform_conversation)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-21T08:19:56.152958Z",
          "iopub.execute_input": "2025-06-21T08:19:56.153603Z",
          "iopub.status.idle": "2025-06-21T08:19:56.191548Z",
          "shell.execute_reply.started": "2025-06-21T08:19:56.153578Z",
          "shell.execute_reply": "2025-06-21T08:19:56.190823Z"
        },
        "colab": {
          "referenced_widgets": [
            "ff48b6e3bf8f498486c9b025966c0e4a"
          ]
        },
        "id": "xGZ2iCES9Un_",
        "outputId": "be095f5e-f612-4efe-e214-eae9e6d85563"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/100 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff48b6e3bf8f498486c9b025966c0e4a"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "merged_test_dataset['text'][0]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-21T08:19:58.864172Z",
          "iopub.execute_input": "2025-06-21T08:19:58.864497Z",
          "iopub.status.idle": "2025-06-21T08:19:58.872632Z",
          "shell.execute_reply.started": "2025-06-21T08:19:58.864473Z",
          "shell.execute_reply": "2025-06-21T08:19:58.872021Z"
        },
        "id": "WN3uuxzE9UoA",
        "outputId": "a923136d-4883-4e79-9563-0b7cc3474207"
      },
      "outputs": [
        {
          "execution_count": 21,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'You are given a list of words, sort them alphabetically ### [\"ant\", \"bat\", \"cat\", \"dog\", \"monkey\"]'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# -------------------- CONFIGURATION --------------------\n",
        "\n",
        "# Model and output\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "new_model = \"tinyllama-finance-chatbot-finetune\"\n",
        "\n",
        "# QLoRA parameters\n",
        "lora_r = 64\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "\n",
        "# BitsAndBytes config\n",
        "use_4bit = True\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "use_nested_quant = False\n",
        "\n",
        "# Training config\n",
        "output_dir = \"./results\"\n",
        "num_train_epochs = 1\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "per_device_train_batch_size = 4\n",
        "per_device_eval_batch_size = 4\n",
        "gradient_accumulation_steps = 1\n",
        "gradient_checkpointing = True\n",
        "max_grad_norm = 0.3\n",
        "learning_rate = 2e-4\n",
        "weight_decay = 0.001\n",
        "optim = \"paged_adamw_32bit\"\n",
        "lr_scheduler_type = \"cosine\"\n",
        "max_steps = -1\n",
        "warmup_ratio = 0.03\n",
        "group_by_length = True\n",
        "save_steps = 0\n",
        "logging_steps = 25\n",
        "max_seq_length = 350\n",
        "\n",
        "# -------------------- TOKENIZER & DATA --------------------\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "def tokenize(example):\n",
        "    encodings = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_seq_length,\n",
        "        return_attention_mask=True,\n",
        "    )\n",
        "    return {\n",
        "        \"input_ids\": encodings[\"input_ids\"],\n",
        "        \"attention_mask\": encodings[\"attention_mask\"],\n",
        "        \"labels\": encodings[\"input_ids\"]\n",
        "    }\n",
        "\n",
        "# Assuming `transformed_dataset` is already defined\n",
        "tokenized_dataset = transformed_dataset.map(\n",
        "    tokenize,\n",
        "    batched=True,\n",
        "    remove_columns=transformed_dataset.column_names,\n",
        "    load_from_cache_file=False\n",
        ")\n",
        "\n",
        "# Set torch format for compatibility\n",
        "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "print(\"âœ… Tokenized keys:\", tokenized_dataset[0].keys())\n",
        "\n",
        "# -------------------- MEMORY MANAGEMENT --------------------\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "os.makedirs(\"/kaggle/working/offload\", exist_ok=True)\n",
        "\n",
        "# -------------------- LOAD MODEL --------------------\n",
        "\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    offload_folder=\"/kaggle/working/offload\",\n",
        "    offload_buffers=True\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# -------------------- PEFT CONFIG --------------------\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# -------------------- TRAINING ARGS --------------------\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "# -------------------- DATA COLLATOR --------------------\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    padding=\"max_length\",\n",
        "    max_length=max_seq_length,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# -------------------- SFT TRAINER --------------------\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    peft_config=peft_config,\n",
        "    args=training_arguments,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# -------------------- TRAIN --------------------\n",
        "\n",
        "trainer.train()\n",
        "print(\"ðŸŽ‰ Training complete!\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-21T09:13:58.427847Z",
          "iopub.execute_input": "2025-06-21T09:13:58.428833Z",
          "iopub.status.idle": "2025-06-21T09:36:35.820571Z",
          "shell.execute_reply.started": "2025-06-21T09:13:58.428799Z",
          "shell.execute_reply": "2025-06-21T09:36:35.819923Z"
        },
        "colab": {
          "referenced_widgets": [
            "6892217bf8524c4dbddfbc5405f95566",
            "35604d410aeb48c7927b0ea1cdf55449"
          ]
        },
        "id": "vyb1trUv9UoA",
        "outputId": "20fa9e28-8d70-4a4b-f413-b3dff1ba325f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6892217bf8524c4dbddfbc5405f95566"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "âœ… Tokenized keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Truncating train dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35604d410aeb48c7927b0ea1cdf55449"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1250/1250 22:28, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>3.467600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.913900</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.645200</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.778100</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.713900</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.683600</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.580900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.635400</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.757400</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.687500</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.760900</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.680600</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>0.651800</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.657300</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>0.775400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.619500</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>0.593800</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.696000</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>0.676100</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.642600</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>0.690900</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.766100</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>0.627300</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.641400</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>0.596200</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.551000</td>\n    </tr>\n    <tr>\n      <td>675</td>\n      <td>0.614700</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.618100</td>\n    </tr>\n    <tr>\n      <td>725</td>\n      <td>0.666400</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.610000</td>\n    </tr>\n    <tr>\n      <td>775</td>\n      <td>0.686800</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.660600</td>\n    </tr>\n    <tr>\n      <td>825</td>\n      <td>0.680500</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.619200</td>\n    </tr>\n    <tr>\n      <td>875</td>\n      <td>0.718300</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.774500</td>\n    </tr>\n    <tr>\n      <td>925</td>\n      <td>0.571100</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.666100</td>\n    </tr>\n    <tr>\n      <td>975</td>\n      <td>0.639200</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.660500</td>\n    </tr>\n    <tr>\n      <td>1025</td>\n      <td>0.664100</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.750500</td>\n    </tr>\n    <tr>\n      <td>1075</td>\n      <td>0.721200</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.505400</td>\n    </tr>\n    <tr>\n      <td>1125</td>\n      <td>0.727500</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.739600</td>\n    </tr>\n    <tr>\n      <td>1175</td>\n      <td>0.620800</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.691400</td>\n    </tr>\n    <tr>\n      <td>1225</td>\n      <td>0.622700</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.508900</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "ðŸŽ‰ Training complete!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Save trained model\n",
        "trainer.model.save_pretrained(new_model)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-21T09:39:06.979363Z",
          "iopub.execute_input": "2025-06-21T09:39:06.980243Z",
          "iopub.status.idle": "2025-06-21T09:39:07.485426Z",
          "shell.execute_reply.started": "2025-06-21T09:39:06.980217Z",
          "shell.execute_reply": "2025-06-21T09:39:07.484868Z"
        },
        "id": "Prz5ET8O9UoC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# List the contents to ensure files are saved\n",
        "print(\"Contents of new_model directory:\", os.listdir(new_model))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-21T09:39:32.312126Z",
          "iopub.execute_input": "2025-06-21T09:39:32.312456Z",
          "iopub.status.idle": "2025-06-21T09:39:32.317379Z",
          "shell.execute_reply.started": "2025-06-21T09:39:32.312416Z",
          "shell.execute_reply": "2025-06-21T09:39:32.316621Z"
        },
        "id": "sjH1FDfF9UoC",
        "outputId": "a8c9caae-da27-4c82-cc47-3aaa619cdbe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Contents of new_model directory: ['adapter_config.json', 'adapter_model.safetensors', 'README.md']\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore warnings\n",
        "import logging\n",
        "from transformers import pipeline\n",
        "from transformers.utils import logging\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "\n",
        "# Run text generation pipeline with our fine-tuned model\n",
        "prompt = \"Generate a title for a blog about the Nobel Prize ceremony.\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=100)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-21T09:41:22.324694Z",
          "iopub.execute_input": "2025-06-21T09:41:22.325262Z",
          "iopub.status.idle": "2025-06-21T09:41:23.107841Z",
          "shell.execute_reply.started": "2025-06-21T09:41:22.325237Z",
          "shell.execute_reply": "2025-06-21T09:41:23.107145Z"
        },
        "id": "LdWIPaDh9UoD",
        "outputId": "0dd8d4cd-15e5-4dce-aa91-e4cd33f1b003"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "<s>[INST] Generate a title for a blog about the Nobel Prize ceremony. [/INST] The Nobel Prize Ceremony: A Tribute to Innovation \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# TensorBoard extension (if applicable)\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir results/runs"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-21T09:41:55.374763Z",
          "iopub.execute_input": "2025-06-21T09:41:55.375292Z",
          "iopub.status.idle": "2025-06-21T09:42:01.938461Z",
          "shell.execute_reply.started": "2025-06-21T09:41:55.375267Z",
          "shell.execute_reply": "2025-06-21T09:42:01.937816Z"
        },
        "id": "qqt3ILAh9UoD",
        "outputId": "27f077bb-6dae-4842-c63d-cc1e7730a6f4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Javascript object>",
            "application/javascript": "\n        (async () => {\n            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n            url.searchParams.set('tensorboardColab', 'true');\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '800');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    "
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload model and tokenizer (if needed) and merge LoRA\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define model_name and new_model\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "new_model = \"tinyllama-finance-chatbot-finetune\"\n",
        "\n",
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Ensure the directory exists\n",
        "if not os.path.exists(new_model):\n",
        "    os.makedirs(new_model)\n",
        "\n",
        "# Define offload directory\n",
        "offload_dir = \"/kaggle/working/\"\n",
        "os.makedirs(offload_dir, exist_ok=True)\n",
        "\n",
        "try:\n",
        "    # Load base model and merge with LoRA\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        low_cpu_mem_usage=True,\n",
        "        return_dict=True,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        offload_folder=offload_dir\n",
        "    )\n",
        "\n",
        "    model = PeftModel.from_pretrained(base_model, new_model)\n",
        "    model = model.merge_and_unload()\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    model.save_pretrained(new_model)\n",
        "    tokenizer.save_pretrained(new_model)\n",
        "\n",
        "    print(\"Contents of new_model directory:\", os.listdir(new_model))\n",
        "\n",
        "except RuntimeError as e:\n",
        "    if \"out of memory\" in str(e):\n",
        "        print(\"Out of memory error. Try using a smaller model or increasing GPU memory.\")\n",
        "        torch.cuda.empty_cache()\n",
        "    else:\n",
        "        raise e\n",
        "except ValueError as e:\n",
        "    print(f\"ValueError: {e}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-21T09:42:45.118321Z",
          "iopub.execute_input": "2025-06-21T09:42:45.118937Z",
          "iopub.status.idle": "2025-06-21T09:42:52.677027Z",
          "shell.execute_reply.started": "2025-06-21T09:42:45.118909Z",
          "shell.execute_reply": "2025-06-21T09:42:52.676355Z"
        },
        "colab": {
          "referenced_widgets": [
            "ff348b8012224442a8f7f8b88a62f70b"
          ]
        },
        "id": "isWrV5nX9UoD",
        "outputId": "aa279128-563e-4683-d5bb-8fe027d210b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3391: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Saving checkpoint shards:   0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff348b8012224442a8f7f8b88a62f70b"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Contents of new_model directory: ['tokenizer.json', 'adapter_config.json', 'special_tokens_map.json', 'tokenizer_config.json', 'adapter_model.safetensors', 'config.json', 'tokenizer.model', 'model.safetensors', 'generation_config.json', 'README.md']\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face Hub authentication\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-21T09:44:28.143120Z",
          "iopub.execute_input": "2025-06-21T09:44:28.143945Z",
          "iopub.status.idle": "2025-06-21T09:44:28.147596Z",
          "shell.execute_reply.started": "2025-06-21T09:44:28.143917Z",
          "shell.execute_reply": "2025-06-21T09:44:28.146809Z"
        },
        "id": "KvgPyFY99UoE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "# âœ… Retrieve token using your actual secret name\n",
        "user_secrets = UserSecretsClient()\n",
        "hf_token = user_secrets.get_secret(\"llm_training\")\n",
        "\n",
        "# Login and push to hub\n",
        "login(token=hf_token)\n",
        "\n",
        "model_repo_name = \"Ayushx29/finance_finetune_model\"\n",
        "tokenizer_repo_name = \"Ayushx29/finance_finetune_model\"\n",
        "\n",
        "model.push_to_hub(model_repo_name, use_auth_token=hf_token, check_pr=True)\n",
        "tokenizer.push_to_hub(tokenizer_repo_name, use_auth_token=hf_token, check_pr=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-21T09:47:58.722571Z",
          "iopub.execute_input": "2025-06-21T09:47:58.723183Z",
          "iopub.status.idle": "2025-06-21T09:48:38.155619Z",
          "shell.execute_reply.started": "2025-06-21T09:47:58.723157Z",
          "shell.execute_reply": "2025-06-21T09:48:38.154971Z"
        },
        "colab": {
          "referenced_widgets": [
            "fd67159c04a644f488b0b90631e8d6a7",
            "0483da08f3ef4451919cdd11e0d0230e",
            "c605d67d6daa44ccaf871cb86a4f77dd",
            "2bf7419023a3493da6faf2634f684850"
          ]
        },
        "id": "exdw8Q9F9UoE",
        "outputId": "6a152b35-8f45-4c0d-804a-d1911b9345ef"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:920: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3391: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Saving checkpoint shards:   0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd67159c04a644f488b0b90631e8d6a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Uploading...:   0%|          | 0.00/2.20G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0483da08f3ef4451919cdd11e0d0230e"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:920: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c605d67d6daa44ccaf871cb86a4f77dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Uploading...:   0%|          | 0.00/500k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2bf7419023a3493da6faf2634f684850"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 82,
          "output_type": "execute_result",
          "data": {
            "text/plain": "CommitInfo(commit_url='https://huggingface.co/Ayushx29/finance_finetune_model/commit/7359821bd1cb6f50c12fb48733784a5be4b05035', commit_message='Upload tokenizer', commit_description='', oid='7359821bd1cb6f50c12fb48733784a5be4b05035', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Ayushx29/finance_finetune_model', endpoint='https://huggingface.co', repo_type='model', repo_id='Ayushx29/finance_finetune_model'), pr_revision=None, pr_num=None)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-21T09:48:46.174132Z",
          "iopub.execute_input": "2025-06-21T09:48:46.174597Z",
          "iopub.status.idle": "2025-06-21T09:48:46.179403Z",
          "shell.execute_reply.started": "2025-06-21T09:48:46.174572Z",
          "shell.execute_reply": "2025-06-21T09:48:46.178899Z"
        },
        "id": "EGhyNhsx9UoE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model back from hub (for inference/verification)\n",
        "fine_tuned_finance_model = AutoModelForCausalLM.from_pretrained(\"Ayushx29/finance_finetune_model\")\n",
        "fine_tuned_tokenizer = AutoTokenizer.from_pretrained(\"Ayushx29/finance_finetune_model\", trust_remote_code=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-21T09:49:10.574541Z",
          "iopub.execute_input": "2025-06-21T09:49:10.574804Z",
          "iopub.status.idle": "2025-06-21T09:49:35.523955Z",
          "shell.execute_reply.started": "2025-06-21T09:49:10.574787Z",
          "shell.execute_reply": "2025-06-21T09:49:35.523342Z"
        },
        "colab": {
          "referenced_widgets": [
            "29eb78b93c114f768f143d24364b49f9",
            "27dbbcab13fd478d9686645b4fe3e33a",
            "f41a60c971c64fa4ae3e09fa0bf5ae30",
            "af5db920edd4413da7811cdf28889847",
            "30f49900a69844e185dc796906a3beee",
            "194e9ae2cc5d4d60a4a079b6a642f60b",
            "c1b2c870172343598ddff3938442b04c"
          ]
        },
        "id": "Fh4vnSTZ9UoF",
        "outputId": "1fd6b65a-2756-47ab-a5fd-01901326cb7a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/674 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29eb78b93c114f768f143d24364b49f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27dbbcab13fd478d9686645b4fe3e33a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f41a60c971c64fa4ae3e09fa0bf5ae30"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af5db920edd4413da7811cdf28889847"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30f49900a69844e185dc796906a3beee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/3.62M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "194e9ae2cc5d4d60a4a079b6a642f60b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1b2c870172343598ddff3938442b04c"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the source (output) directory and the new target directory\n",
        "source_dir = \"/kaggle/working/tinyllama-finance-chatbot-finetune\"\n",
        "target_dir = \"/kaggle/working/new_dir\"\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "files_to_copy = [\n",
        "    \"config.json\",\n",
        "    \"tokenizer.json\",\n",
        "    \"tokenizer_config.json\",\n",
        "    \"adapter_model.safetensors\",\n",
        "    \"generation_config.json\",\n",
        "    \"tokenizer.model\",\n",
        "    \"special_tokens_map.json\"\n",
        "]\n",
        "for file_name in files_to_copy:\n",
        "    src = os.path.join(source_dir, file_name)\n",
        "    dst = os.path.join(target_dir, file_name)\n",
        "    if os.path.exists(src):\n",
        "        shutil.copy2(src, dst)\n",
        "        print(f\"Copied {file_name} to {target_dir}\")\n",
        "    else:\n",
        "        print(f\"{file_name} not found in {source_dir}\")\n",
        "\n",
        "print(\"âœ… File copying complete.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-21T09:55:41.559426Z",
          "iopub.execute_input": "2025-06-21T09:55:41.560057Z",
          "iopub.status.idle": "2025-06-21T09:55:41.947903Z",
          "shell.execute_reply.started": "2025-06-21T09:55:41.560035Z",
          "shell.execute_reply": "2025-06-21T09:55:41.947117Z"
        },
        "id": "s3zkyPjO9UoF",
        "outputId": "a92773a4-8c51-44f9-db74-21086742039f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Copied config.json to /kaggle/working/new_dir\nCopied tokenizer.json to /kaggle/working/new_dir\nCopied tokenizer_config.json to /kaggle/working/new_dir\nCopied adapter_model.safetensors to /kaggle/working/new_dir\nCopied generation_config.json to /kaggle/working/new_dir\nCopied tokenizer.model to /kaggle/working/new_dir\nCopied special_tokens_map.json to /kaggle/working/new_dir\nâœ… File copying complete.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}